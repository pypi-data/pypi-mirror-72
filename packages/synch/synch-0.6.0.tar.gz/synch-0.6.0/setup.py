# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['synch', 'synch.broker', 'synch.reader', 'synch.replication']

package_data = \
{'': ['*']}

install_requires = \
['clickhouse-driver',
 'kafka-python',
 'mysql-replication',
 'mysqlclient',
 'psycopg2',
 'pydantic',
 'python-dateutil',
 'redis',
 'sentry-sdk',
 'sqlparse']

entry_points = \
{'console_scripts': ['synch = synch.cli:cli']}

setup_kwargs = {
    'name': 'synch',
    'version': '0.6.0',
    'description': 'Sync data from other DB to ClickHouse, current support postgres and mysql, and support full and increment ETL.',
    'long_description': '# Synch\n\n![pypi](https://img.shields.io/pypi/v/synch.svg?style=flat)\n![docker](https://img.shields.io/docker/cloud/build/long2ice/synch)\n![license](https://img.shields.io/github/license/long2ice/synch)\n![workflows](https://github.com/long2ice/synch/workflows/pypi/badge.svg)\n\n[中文文档](https://blog.long2ice.cn/2020/05/synch%E4%B8%80%E4%B8%AA%E5%90%8C%E6%AD%A5mysql%E6%95%B0%E6%8D%AE%E5%88%B0clickhouse%E7%9A%84%E9%A1%B9%E7%9B%AE/)\n\n## Introduction\n\nSync data from other DB to ClickHouse, current support postgres and mysql, and support full and increment ETL.\n\n![synch](https://github.com/long2ice/synch/raw/dev/images/synch.png)\n\n## Features\n\n- Full data etl and real time increment etl.\n- Support DDL and DML sync, current support `add column` and `drop column` and `change column` of DDL, and full support of DML also.\n- Custom configurable items.\n- Support kafka and redis as broker.\n\n## Requirements\n\n- [redis](https://redis.io), cache mysql binlog file and position and as broker, support redis cluster also.\n- [kafka](https://kafka.apache.org), need if you use kafka as broker.\n- [clickhouse-jdbc-bridge](https://github.com/ClickHouse/clickhouse-jdbc-bridge), need if you use postgres and set `auto_full_etl = True`, or exec `synch etl` command.\n\n## Install\n\n```shell\n> pip install synch\n```\n\n## Usage\n\n### synch.ini\n\nsynch will read default config from `./synch.ini`, or you can use `synch -c` specify config file.\n\n**Don\'t delete any section in synch.ini although you don\'t need it, just keep default as it.**\n\n```ini\n[core]\n# when set True, will display sql information.\ndebug = True\n# current support redis and kafka\nbroker_type = redis\n# source database, current support mysql and postgres\nsource_db = mysql\n# these tables skip delete, multiple separated with comma, format with schema.table\nskip_delete_tables =\n# these tables skip update, multiple separated with comma, format with schema.table\nskip_update_tables =\n# skip delete or update dmls, multiple separated with comma, example: delete,update\nskip_dmls =\n# how many num to submit,recommend set 20000 when production\ninsert_num = 1\n# how many seconds to submit,recommend set 60 when production\ninsert_interval = 1\n# auto do full etl at first when table not exists\nauto_full_etl = True\n\n[sentry]\n# sentry environment\nenvironment = development\n# sentry dsn\ndsn =\n\n[redis]\nhost = 127.0.0.1\nport = 6379\npassword =\ndb = 0\nprefix = synch\n# enable redis sentinel\nsentinel = False\n# redis sentinel hosts,multiple separated with comma\nsentinel_hosts = 127.0.0.1:5000,127.0.0.1:5001,127.0.0.1:5002\nsentinel_master = master\n# stream max len, will delete redundant ones with FIFO\nqueue_max_len = 200000\n\n[mysql]\nserver_id = 1\n# optional, read from `show master status` result if empty\ninit_binlog_file =\n# optional, read from `show master status` result if empty\ninit_binlog_pos =\nhost = mysql\nport = 3306\nuser = root\npassword = 123456\n\n# sync schema, format with mysql.schema, each schema for one section.\n[mysql.test]\n# multiple separated with comma\ntables = test\n# kafka partition, need when broker_type=kafka\nkafka_partition = 0\n\n# when source_db = postgres\n[postgres]\nhost = postgres\nport = 5432\nuser = postgres\npassword =\n\n[postgres.postgres]\ntables = test\nkafka_partition = 0\n\n[clickhouse]\nhost = 127.0.0.1\nport = 9000\nuser = default\npassword =\n\n# need when broker_type=kafka\n[kafka]\n# kafka servers,multiple separated with comma\nservers = 127.0.0.1:9092\ntopic = synch\n```\n\n### Full data etl\n\nMaybe you need make full data etl before continuous sync data from MySQL to ClickHouse or redo data etl with `--renew`.\n\n```shell\n> synch etl -h\n\nusage: synch etl [-h] --schema SCHEMA [--tables TABLES] [--renew]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --schema SCHEMA  Schema to full etl.\n  --tables TABLES  Tables to full etl,multiple tables split with comma,default read from environment.\n  --renew          Etl after try to drop the target tables.\n```\n\nFull etl from table `test.test`:\n\n```shell\n> synch etl --schema test --tables test\n```\n\n### Produce\n\nListen all MySQL binlog and produce to broker.\n\n```shell\n> synch produce\n```\n\n### Consume\n\nConsume message from broker and insert to ClickHouse,and you can skip error rows with `--skip-error`. And synch will do full etl at first when set `auto_full_etl = True` in `synch.ini`.\n\n```shell\n> synch consume -h\n\nusage: synch consume [-h] --schema SCHEMA [--skip-error] [--last-msg-id LAST_MSG_ID]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --schema SCHEMA       Schema to consume.\n  --skip-error          Skip error rows.\n  --last-msg-id LAST_MSG_ID\n                        Redis stream last msg id or kafka msg offset, depend on broker_type in config.\n```\n\nConsume schema `test` and insert into `ClickHouse`:\n\n```shell\n> synch consume --schema test\n```\n\n## Use docker-compose(recommended)\n\n<details>\n<summary>Redis Broker, lightweight and for low concurrency</summary>\n\n```yaml\nversion: "3"\nservices:\n  producer:\n    depends_on:\n      - redis\n    image: long2ice/synch\n    command: synch produce\n    volumes:\n      - ./synch.ini:/synch/synch.ini\n  consumer.test:\n    depends_on:\n      - redis\n    image: long2ice/synch\n    command: synch consume --schema test\n    volumes:\n      - ./synch.ini:/synch/synch.ini\n  redis:\n    hostname: redis\n    image: redis:latest\n    volumes:\n      - redis\nvolumes:\n  redis:\n```\n\n</details>\n\n<details>\n<summary>Kafka Broker, for high concurrency</summary>\n\n```yml\nversion: "3"\nservices:\n  zookeeper:\n    image: bitnami/zookeeper:3\n    hostname: zookeeper\n    environment:\n      - ALLOW_ANONYMOUS_LOGIN=yes\n    volumes:\n      - zookeeper:/bitnami\n  kafka:\n    image: bitnami/kafka:2\n    hostname: kafka\n    environment:\n      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181\n      - ALLOW_PLAINTEXT_LISTENER=yes\n      - JMX_PORT=23456\n      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true\n      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092\n    depends_on:\n      - zookeeper\n    volumes:\n      - kafka:/bitnami\n  kafka-manager:\n    image: hlebalbau/kafka-manager\n    ports:\n      - "9000:9000"\n    environment:\n      ZK_HOSTS: "zookeeper:2181"\n      KAFKA_MANAGER_AUTH_ENABLED: "false"\n    command: -Dpidfile.path=/dev/null\n  producer:\n    depends_on:\n      - redis\n      - kafka\n      - zookeeper\n    image: long2ice/synch\n    command: synch produce\n    volumes:\n      - ./synch.ini:/synch/synch.ini\n  consumer.test:\n    depends_on:\n      - redis\n      - kafka\n      - zookeeper\n    image: long2ice/synch\n    command: synch consume --schema test\n    volumes:\n      - ./synch.ini:/synch/synch.ini\n  redis:\n    hostname: redis\n    image: redis:latest\n    volumes:\n      - redis:/data\nvolumes:\n  redis:\n  kafka:\n  zookeeper:\n```\n\n</details>\n\n## Important\n\n- Synch don\'t support composite primary key, and you need always keep a primary key or unique key.\n- DDL sync not support postgres.\n- Postgres sync is not fully test, be careful use it in production.\n\n## Optional\n\n[Sentry](https://github.com/getsentry/sentry), error reporting, worked if set `dsn` in config.\n\n## QQ Group\n\n<img width="200" src="https://github.com/long2ice/synch/raw/dev/images/qq_group.png"/>\n\n## Support this project\n\n- Just give a star!\n- Join QQ group for communication.\n- Donation.\n\n### AliPay\n\n<img width="200" src="https://github.com/long2ice/synch/raw/dev/images/alipay.jpeg"/>\n\n### WeChat Pay\n\n<img width="200" src="https://github.com/long2ice/synch/raw/dev/images/wechatpay.jpeg"/>\n\n### PayPal\n\nDonate money by [paypal](https://www.paypal.me/long2ice) to my account long2ice.\n\n## ThanksTo\n\nPowerful Python IDE [Pycharm](https://www.jetbrains.com/pycharm/?from=synch) from [Jetbrains](https://www.jetbrains.com/?from=synch).\n\n![jetbrains](https://github.com/long2ice/synch/raw/dev/images/jetbrains.svg)\n\n## License\n\nThis project is licensed under the [Apache-2.0](https://github.com/long2ice/synch/blob/master/LICENSE) License.\n',
    'author': 'long2ice',
    'author_email': 'long2ice@gmail.com',
    'maintainer': None,
    'maintainer_email': None,
    'url': 'https://github.com/long2ice/synch',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'entry_points': entry_points,
    'python_requires': '>=3.6,<4.0',
}


setup(**setup_kwargs)
