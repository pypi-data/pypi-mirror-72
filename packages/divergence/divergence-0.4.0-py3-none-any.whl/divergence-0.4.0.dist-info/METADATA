Metadata-Version: 2.1
Name: divergence
Version: 0.4.0
Summary: Information Theoretic Measures of Entropy and Divergence
Home-page: https://github.com/michaelnowotny/divergence
Author: Michael Christoph Nowotny
Author-email: nowotnym@gmail.com
License: MIT
Platform: UNKNOWN
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: Implementation :: CPython
Requires-Python: >=3.6.0
Description-Content-Type: text/markdown
Requires-Dist: numba
Requires-Dist: numpy
Requires-Dist: pytest
Requires-Dist: quadpy
Requires-Dist: scipy
Requires-Dist: statsmodels


# Divergence
Divergence is a Python package to compute statistical measures of entropy and divergence from probability distributions and samples.

The following functionality is provided:
* (Information) Entropy
* Cross Entropy 
* Relative Entropy or Kullback-Leibler Divergence
* Jensen-Shannon Divergence
* Mutual Information (for continuous random variables)

The units in which these entropy and divergence measures are calculated can be specified by the user. 
This is achieved by the argument `log_fun`, which accepts a function that calculates the logarithm with respect to a particular base. 
The following units can be realized by the corresponding choice of the argument `log_fun` in the entropy and divergence calculation functions:
* bits: base 2 via `np.log2`
* nats: base e via `np.log`
* dits: base 10 via `np.log10`

In a Bayesian context, relative entropy can be used as a measure of the information gained by moving 
from a prior distribution `q` to a posterior distribution `p`.

## Installation

<pre>
    pip install divergence
</pre>

## Examples
See the Jupyter notebook [Divergence](https://github.com/michaelnowotny/divergence/blob/master/notebooks/Divergence.ipynb).


